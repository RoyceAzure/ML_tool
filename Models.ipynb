{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'eli5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bc9a328dcf66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0meli5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0meli5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPermutationImportance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'eli5'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,recall_score,classification_report , accuracy_score , average_precision_score, f1_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import eli5 \n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import pydot #pip install pydot\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from scipy.stats import skew\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.ensemble import StackingClassifier, StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2046cb22ef68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprinting_Kfold_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "best_c = printing_Kfold_scores(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-cfb229fa9127>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m def plot_confusion_matrix(cm, classes,\n\u001b[0;32m      2\u001b[0m                           \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Confusion matrix'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                           cmap=plt.cm.Blues):\n\u001b[0m\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m      5\u001b[0m     \u001b[0m绘制混淆矩阵\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    绘制混淆矩阵\n",
    "    \"\"\"\n",
    "#     plt.figure(figsize = (50,50))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Regression_eva_plot(y_pred, y_test):\n",
    "    figsize(8, 8)\n",
    "    # Density plot of the final predictions and the test values\n",
    "    sns.kdeplot(y_pred, label = 'Predictions')\n",
    "    sns.kdeplot(y_test, label = 'Values')\n",
    "\n",
    "    # Label the plot\n",
    "    plt.xlabel('Target'); plt.ylabel('Density');\n",
    "    plt.title('Test Values and Predictions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Regression_rmse_cv(model,X_test,y_test,class_names):\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "    print(\"mean rmse: \",np.mean(rmse) )\n",
    "    Regression_eva_plot(y_pred, y_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Regression_error_displot(y_pred, y_test):\n",
    "    figsize = (6, 6)\n",
    "\n",
    "    # Calculate the residuals \n",
    "    residuals = y_pred - y_test\n",
    "\n",
    "    # Plot the residuals in a histogram\n",
    "    plt.hist(residuals, color = 'red', bins = 20,\n",
    "             edgecolor = 'black')\n",
    "    plt.xlabel('Error'); plt.ylabel('Count')\n",
    "    plt.title('Distribution of Residuals');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classify_evaluate(model, X_test,y_test, class_names):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    print(\"召回率: \", recall_score(y_test.values, y_pred,average='macro') ) #多分類用macro   二分類去掉\n",
    "    print(\"ACC: \", accuracy_score(y_test.values, y_pred) )\n",
    "#     print(\"精確: \", average_precision_score(y_test.values, y_pred,average='macro') )\n",
    "    print(\"F1: \", f1_score(y_test.values, y_pred,average='macro') )\n",
    "#     print(\"average_precision_score: \", average_precision_score(y_test.values, y_pred))  muticlass is not supported\n",
    "    # 绘制\n",
    "    class_names = class_names\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix\n",
    "                          , classes=class_names\n",
    "                          , title='Confusion matrix')\n",
    "    plt.show()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型檢視"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PermutationImportance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-145386f3df58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mperm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPermutationImportance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0meli5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PermutationImportance' is not defined"
     ]
    }
   ],
   "source": [
    "perm = PermutationImportance(lr, random_state=1).fit(X_test, y_test.values.reshape(-1, 1))\n",
    "eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Importance_feature(model,feature_list):\n",
    "    importances = list(model.feature_importances_)\n",
    "\n",
    "# 名字，数值组合在一起\n",
    "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "    # 排序\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "    # 打印出来 \n",
    "    [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "    plt.style.use('fivethirtyeight')\n",
    "#     plt.figure(figsize=(5*len(importances),30))\n",
    "    # 指定位置\n",
    "    x_values = list(range(len(importances)))\n",
    "\n",
    "    # 绘图\n",
    "    plt.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n",
    "\n",
    "    # x轴名字得竖着写\n",
    "    plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "\n",
    "    # 图名\n",
    "    plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Importance_feature(best_grid_rf, X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 樹可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拿到其中的一棵树\n",
    "tree = best_grid_rf2.estimators_[5]\n",
    "\n",
    "# 导出成dot文件\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "\n",
    "# 绘图\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "\n",
    "# 展示\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graph(clf, feature_names, class_name):\n",
    "    \"\"\"Print decision tree.\"\"\"\n",
    "    graph = export_graphviz(\n",
    "        clf,\n",
    "        label=\"root\",\n",
    "        proportion=True,\n",
    "        impurity=False, \n",
    "        out_file=None, \n",
    "        feature_names=feature_names,\n",
    "        class_names=class_name,\n",
    "        filled=True,\n",
    "        rounded=True,\n",
    "        precision = 1\n",
    "    )\n",
    "    (graph, ) = pydot.graph_from_dot_data(graph)\n",
    "#     graph = pydotplus.graph_from_dot_data(graph)  \n",
    "    return Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_graph(tree, feature_list, {0:\"dead\", 1:\"lives\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printing_Kfold_scores(X,Y):\n",
    "    y_train_data =Y.copy()\n",
    "    x_train_data = X.copy()\n",
    "    \n",
    "    y_train_data.reset_index(drop = True, inplace = True)\n",
    "    x_train_data.reset_index(drop = True, inplace = True)\n",
    "    print(x_train_data)\n",
    "    print(y_train_data)\n",
    "    fold = KFold(5,shuffle=False) \n",
    "\n",
    "    # 定义不同力度的正则化惩罚力度\n",
    "    c_param_range = [0.01,0.1,1,10,100]\n",
    "    # 展示结果用的表格\n",
    "    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n",
    "    results_table['C_parameter'] = c_param_range\n",
    "\n",
    "    # k-fold 表示K折的交叉验证，这里会得到两个索引集合: 训练集 = indices[0], 验证集 = indices[1]\n",
    "    j = 0\n",
    "    #循环遍历不同的参数\n",
    "    for c_param in c_param_range:\n",
    "        print('-------------------------------------------')\n",
    "        print('正则化惩罚力度: ', c_param)\n",
    "        print('-------------------------------------------')\n",
    "        print('')\n",
    "\n",
    "        recall_accs = []\n",
    "        \n",
    "        #一步步分解来执行交叉验证\n",
    "        for iteration, (train_index, test_index) in enumerate(fold.split(x_train_data),start=1):   #indice[0] = train_index , indices[1] = test_index \n",
    "\n",
    "            # 指定算法模型，并且给定参数\n",
    "#             lr = LogisticRegression(solver = \"liblinear\",C = c_param, penalty = 'l1')\n",
    "            lr = LogisticRegression(solver = \"newton-cg\",C = c_param, penalty = 'l2')\n",
    "#             lr = LogisticRegression(solver = \"liblinear\",C = c_param, penalty = 'l1', random_state=0)\n",
    "            # 训练模型，注意索引不要给错了，训练的时候一定传入的是训练集，所以X和Y的索引都是0\n",
    "#             print(train_index)\n",
    "#             print(test_index)\n",
    "            X_train, X_test = x_train_data.loc[train_index,:], x_train_data.loc[test_index,:]\n",
    "            y_train, y_test = y_train_data.loc[train_index], y_train_data.loc[test_index]\n",
    "#             print(y_train)\n",
    "            lr.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "            # 建立好模型后，预测模型结果，这里用的就是验证集，索引为1\n",
    "            y_pred = lr.predict(X_test.values)\n",
    "\n",
    "            # 有了预测结果之后就可以来进行评估了，这里recall_score需要传入预测值和真实值。\n",
    "            recall_acc = recall_score(y_test.values,y_pred, average='macro')   #多分類用macro   二分類去掉\n",
    "            # 一会还要算平均，所以把每一步的结果都先保存起来。\n",
    "            recall_accs.append(recall_acc)\n",
    "            print('Iteration ', iteration,': 召回率 = ', recall_acc)\n",
    "\n",
    "        # 当执行完所有的交叉验证后，计算平均结果\n",
    "        results_table.loc[j,'Mean recall score'] = np.mean(recall_accs)\n",
    "        j += 1\n",
    "        print('')\n",
    "        print('平均召回率 ', np.mean(recall_accs))\n",
    "        print('')\n",
    "#         print(results_table)\n",
    "    #找到最好的参数，哪一个Recall高，自然就是最好的了。\n",
    "    best_c = results_table.loc[results_table['Mean recall score'].astype('float32').idxmax()]['C_parameter']\n",
    "    \n",
    "    # 打印最好的结果\n",
    "    print('*********************************************************************************')\n",
    "    print('效果最好的模型所选参数 = ', best_c)\n",
    "    print('*********************************************************************************')\n",
    "    \n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c = printing_Kfold_scores(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver = \"newton-cg\",C = best_c, penalty = 'l2')\n",
    "lr.fit(X_train,y_train.values.ravel())\n",
    "y_pred = Classify_evaluate(lr, X_test,y_test, lb_Y.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\"solver\" : [\"newton-cg\"],\"C\":np.logspace(-3,3,7), \"penalty\":[\"l2\"]}# l1 lasso l2 ridge\n",
    "lr=LogisticRegression()\n",
    "grid_lr=GridSearchCV(lr,params,cv=10)\n",
    "grid_lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = grid_lr.best_estimator_\n",
    "y_pred = Classify_evaluate(best_lr, X_test,y_test, lb_Y.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d9b495ef68aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#scores = ['precision', 'recall']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m grid_svm = GridSearchCV(\n\u001b[0m\u001b[0;32m      7\u001b[0m     SVC(), tuned_parameters, scoring='f1_macro')\n\u001b[0;32m      8\u001b[0m \u001b[0mgrid_svm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "#scores = ['precision', 'recall']\n",
    "\n",
    "grid_svm = GridSearchCV(\n",
    "    SVC(), tuned_parameters, scoring='f1_macro')\n",
    "grid_svm.fit(X_train, y_train)\n",
    "grid_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svm = grid_svm.best_estimator_\n",
    "y_pred = Classify_evaluate(best_svm, X_test,y_test, lb_Y.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nb = GaussianNB()  \n",
    "Nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Classify_evaluate(Nb, X_test,y_test, lb_Y.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立树的个数\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# 最大特征的选择方式\n",
    "max_features = ['auto', 'sqrt']\n",
    "# 树的最大深度\n",
    "max_depth = [int(x) for x in np.linspace(10, 20, num = 2)]\n",
    "max_depth.append(None)\n",
    "# 节点最小分裂所需样本个数\n",
    "min_samples_split = [2, 5, 10]\n",
    "# 叶子节点最小样本数，任何分裂不能让其子节点样本数少于此值\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# 样本采样方法\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "#訓練樹\n",
    "# 随机选择最合适的参数组合\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 100, scoring='recall', \n",
    "                              cv = 3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# 执行寻找操作\n",
    "rf_random.fit(X_train, y_train)\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "y_pred = Classify_evaluate(best_random, X_test,y_test, lb_Y.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络搜索\n",
    "# param_grid = {\n",
    "#     'bootstrap': [True],\n",
    "#     'max_depth': [8,10,12],\n",
    "#     'max_features': ['auto'],\n",
    "#     'min_samples_leaf': [2,3, 4, 5,6],\n",
    "#     'min_samples_split': [3, 5, 7],\n",
    "#     'n_estimators': [800, 900, 1000, 1200]\n",
    "# }\n",
    "param_grid2 = {\n",
    "    'bootstrap': [False],\n",
    "    'min_samples_leaf': [2],\n",
    "    'max_features': ['auto'],\n",
    "    'max_depth': [10],\n",
    "    'min_samples_split': [17],\n",
    "    'n_estimators': [400]\n",
    "}\n",
    "# 选择基本算法模型\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# 网络搜索\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid2, \n",
    "                           scoring = 'f1', cv = 3, \n",
    "                           n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train,y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid_rf = grid_search.best_estimator_\n",
    "Classify_evaluate(best_grid_rf,X_test,y_test, class_names = label_le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\n",
    "#     \"max_depth\":[10,15],\n",
    "#     \"learning_rate\": [0.01,0.02],\n",
    "#     \"feature_fraction\":[0.9,1.2,1.5],\n",
    "#     \"bagging_fraction\" : [0.3,0.5,0.6],\n",
    "#     \"bagging_freq\": [2,3,4],\n",
    "#     \"lambda_l1\" : [0,0.1],\n",
    "#     \"lambda_l2\" : [0,5,7,10],\n",
    "#     \"cat_smooth\" : [5,7,10],\n",
    "#     \"n_estimators\" :[100000, 150000],\n",
    "#     \"objective\" : ['binary'],\n",
    "#     \"class_weight\" : ['balanced'],\n",
    "#     \"reg_alpha\" : [0.1], \n",
    "#      \"reg_lambda\" : [0.1],\n",
    "#     \"subsample\" : [0.8],\n",
    "#      \"n_jobs\" : [-1], \"random_state\" : [50]\n",
    "# }\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier()\n",
    "parameters = {\n",
    "    \"n_estimators\" : [10000],\n",
    "    'max_depth': [15],\n",
    "    'bagging_fraction': [0.9],\n",
    "    'bagging_freq': [6],\n",
    "    \"feature_fraction\":[0.9],\n",
    "    \"objective\" : ['binary'],\n",
    "    \"class_weight\" : ['balanced'],\n",
    "    'cat_smooth': [5],\n",
    "    'feature_fraction': [0.9],\n",
    "    'lambda_l1': [0.1],\n",
    "    'lambda_l2': [7],\n",
    "    'learning_rate': [0.05],\n",
    "    \"reg_alpha\" : [0.1], \n",
    "     \"reg_lambda\" : [0.1],\n",
    "    \"subsample\" : [0.8],\n",
    "     \"n_jobs\" : [-1], \"random_state\" : [50]\n",
    "    }\n",
    "lgb_model_search = GridSearchCV(lgb_model, parameters, cv =3, scoring = 'accuracy', verbose = 2, n_jobs = -1)\n",
    "lgb_model_search.fit(X_train, y_train)\n",
    "lgb_model_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgb = lgb_model_search.best_estimator_\n",
    "y_pred = Classify_evaluate(best_lgb, X_test,y_test, lb_Y.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "#brute force scan for all parameters, here are the tricks\n",
    "#usually max_depth is 6,7,8\n",
    "#learning rate is around 0.05, but small changes may make big diff\n",
    "#tuning min_child_weight subsample colsample_bytree can have \n",
    "#much fun of fighting against overfit \n",
    "#n_estimators is how many round of boosting\n",
    "#finally, ensemble xgboost with multiple seeds may reduce variance\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.05], #so called `eta` value\n",
    "              'max_depth': [8],   #max_depth [default=6] range: [0,∞] (0 is only accepted in lossguided growing policy when tree_method is set as hist)\n",
    "              'min_child_weight': [6], # [default=1] range: [0,∞]\n",
    "              \"gamma\" : [0.4] , #default=0, range: [0,∞]  降低你和風險\n",
    "              'subsample': [0.5],  # [default=1] range: (0,1]\n",
    "              'colsample_bytree': [0.5],\n",
    "              'n_estimators': [5000], #number of trees, change it to 1000 for better results\n",
    "              \"booster\":['gbtree'],\n",
    "              'seed': [1337]}\n",
    "\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb_model, parameters, n_jobs=5, \n",
    "                   cv=5,   \n",
    "                   scoring='f1_macro',    #muticlass use f1_macro ,other use acc\n",
    "                   verbose=2, refit=True)\n",
    "xgb_grid.fit(X_train,y_train)\n",
    "xgb_grid.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = xgb_grid.best_estimator_\n",
    "y_pred = Classify_evaluate(best_xgb, X_test,y_test, lb_Y.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_grid = {'n_estimators': [100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800]}\n",
    "\n",
    "model = GradientBoostingRegressor(loss = 'lad', max_depth = 5,\n",
    "                                  min_samples_leaf = 6,\n",
    "                                  min_samples_split = 6,\n",
    "                                  max_features = None,\n",
    "                                  random_state = 42)\n",
    "\n",
    "# Grid Search Object using the trees range and the random forest model\n",
    "grad_search = GridSearchCV(estimator = model, param_grid=trees_grid, cv = 4, \n",
    "                           scoring = 'neg_mean_absolute_error', verbose = 1,\n",
    "                           n_jobs = -1, return_train_score = True)\n",
    "grid_search.fit(X, train_labels)\n",
    "grid_search.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grad = grad_search.best_estimator_\n",
    "y_pred = Classify_evaluate(best_xgb, X_test,y_test, lb_Y.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2fdd33de90dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mstacking\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRegressorMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmeta_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "class stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self,mod,meta_model):\n",
    "        self.mod = mod\n",
    "        self.meta_model = meta_model\n",
    "        self.kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.saved_model = [list() for i in self.mod]\n",
    "        oof_train = np.zeros((X.shape[0], len(self.mod)))\n",
    "        \n",
    "        for i,model in enumerate(self.mod):\n",
    "            for train_index, val_index in self.kf.split(X,y):\n",
    "                renew_model = clone(model)\n",
    "                renew_model.fit(X[train_index], y[train_index])\n",
    "                self.saved_model[i].append(renew_model)\n",
    "                oof_train[val_index,i] = renew_model.predict(X[val_index])\n",
    "        \n",
    "        self.meta_model.fit(oof_train,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n",
    "                                      for single_model in self.saved_model]) \n",
    "        return self.meta_model.predict(whole_test)\n",
    "    \n",
    "    def get_oof(self,X,y,test_X):\n",
    "        oof = np.zeros((X.shape[0],len(self.mod)))\n",
    "        test_single = np.zeros((test_X.shape[0],5))\n",
    "        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n",
    "        for i,model in enumerate(self.mod):\n",
    "            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n",
    "                clone_model = clone(model)\n",
    "                clone_model.fit(X[train_index],y[train_index])\n",
    "                oof[val_index,i] = clone_model.predict(X[val_index])\n",
    "                test_single[:,j] = clone_model.predict(test_X)\n",
    "            test_mean[:,i] = test_single.mean(axis=1)\n",
    "        return oof, test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stacking regression models\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    models = dict()\n",
    "    models['lr'] = LogisticRegression()\n",
    "    models['Rf'] = RandomForestClassifier()\n",
    "    models['xgb'] = XGBClassifier()\n",
    "    models['svm'] = SVC()\n",
    "    models['bayes'] = GaussianNB()\n",
    "    models['lgb'] = lgb.LGBMClassifier()\n",
    "    models['stacking'] = get_stacking()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_stacking():\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('lr', LogisticRegression()))\n",
    "    level0.append(('Rf',RandomForestClassifier()))\n",
    "    level0.append(('xgb', XGBClassifier()))\n",
    "    level0.append(('svm', SVC()))\n",
    "    level0.append(('bayes', GaussianNB()))\n",
    "#     level0.append(('lgb', lgb.LGBMClassifier()))\n",
    "    # define meta learner model\n",
    "    level1 = lgb.LGBMClassifier()\n",
    "    # define the stacking ensemble\n",
    "    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a stacking ensemble of models\n",
    "def get_best_stacking():\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('lr', lr))\n",
    "    level0.append(('Rf', best_random))\n",
    "    level0.append(('xgb', best_xgb))\n",
    "    level0.append(('svm', best_svm))\n",
    "    level0.append(('bayes', Nb))\n",
    "    level0.append(('lgb', best_lgb))\n",
    "    # define meta learner model\n",
    "    level1 = LogisticRegression()\n",
    "    # define the stacking ensemble\n",
    "    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='f1_macro', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X_train, y_train)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_best_stacking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Classify_evaluate(model, X_test,y_test, lb_Y.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回歸stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    models = dict()\n",
    "    models['lr'] = LinearRegression()\n",
    "    models['Rf'] = RandomForestRegressor()\n",
    "    models['xgb'] = XGBRegressor()\n",
    "    models['svm'] = SVR(C = 1000, gamma = 0.1)\n",
    "    models['bayes'] = BayesianRidge()\n",
    "    models['lgb'] = lgb.LGBMRegressor()\n",
    "    models['stacking'] = get_base_stacking()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_stacking():\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('lr', LinearRegression()))\n",
    "    level0.append(('Rf',RandomForestRegressor()))\n",
    "    level0.append(('xgb', XGBRegressor()))\n",
    "    level0.append(('svm', SVR(C = 1000, gamma = 0.1)))\n",
    "    level0.append(('bayes', BayesianRidge()))\n",
    "    level0.append(('lgb', lgb.LGBMRegressor()))\n",
    "    # define meta learner model\n",
    "    level1 = lgb.LGBMRegressor()\n",
    "    # define the stacking ensemble\n",
    "    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
